import { useCallback, useEffect, useRef, useState } from "react";
import { GoogleGenAI, LiveServerMessage, Session, Modality, MediaResolution } from "@google/genai";

// --- Constantes ---
const SEND_SAMPLE_RATE = 16000;
const CHUNK_SIZE = 1024;
const MAX_RECONNECT_ATTEMPTS = 3;
const TOKEN_EXPIRATION_MINUTES = 25;
const MODEL_NAME = "models/gemini-2.5-flash-native-audio-preview-09-2025"; // ATUALIZADO: Modelo exato do c√≥digo oficial.

// --- Tipos ---
interface UseGeminiLiveAPIProps {
  systemInstruction?: string;
  onMessage?: (text: string) => void;
  onAudio?: (audioBlob: Blob) => void;
}

// --- Cache de Token ---
let cachedToken: { token: string; expiresAt: number } | null = null;

/**
 * Hook para interagir com a Gemini Live API (√Åudio Nativo) num ambiente de NAVEGADOR.
 * Esta implementa√ß√£o √© 100% focada no fluxo de eventos do browser, abandonando
 * os padr√µes de Node.js (handleTurn/waitMessage) que causavam os erros.
 */
export function useGeminiLiveAPI({
  systemInstruction = "Seja um assistente de IA conversacional.",
  onMessage,
  onAudio,
}: UseGeminiLiveAPIProps) {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const sessionRef = useRef<Session | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const reconnectAttemptsRef = useRef(0);

  // --- 1. Processamento de Respostas do Servidor ---
  const handleServerMessage = useCallback(
    (message: LiveServerMessage) => {
      try {
        if (message.serverContent?.modelTurn?.parts) {
          const part = message.serverContent.modelTurn.parts[0];
          if (part.text) {
            console.log("üí¨ Texto recebido:", part.text);
            onMessage?.(part.text);
          }
          if (part.inlineData?.data && part.inlineData.mimeType) {
            const audioData = part.inlineData.data;
            const mimeType = part.inlineData.mimeType;
            console.log(`üîä √Åudio recebido (${mimeType}, ${audioData.length} bytes)`);
            
            const byteCharacters = atob(audioData);
            const byteNumbers = new Array(byteCharacters.length);
            for (let i = 0; i < byteCharacters.length; i++) {
              byteNumbers[i] = byteCharacters.charCodeAt(i);
            }
            const byteArray = new Uint8Array(byteNumbers);
            const audioBlob = new Blob([byteArray], { type: mimeType });
            onAudio?.(audioBlob);
          }
        }
        if (message.serverContent?.turnComplete) {
          console.log("‚úÖ Turno do modelo completo.");
        }
      } catch (e) {
        console.error("‚ùå Erro ao processar mensagem do servidor:", e);
      }
    },
    [onMessage, onAudio]
  );

  // --- 2. Conex√£o com a API ---
  const connect = useCallback(async () => {
    if (sessionRef.current || isLoading) return;

    setIsLoading(true);
    setError(null);
    console.log("üîå Conectando √† Live API...");

    try {
      if (!cachedToken || Date.now() > cachedToken.expiresAt) {
        console.log("üîë Obtendo novo token ephemeral...");
        const response = await fetch("/api/auth/ephemeral-token", { method: "POST" });
        if (!response.ok) throw new Error(`Falha ao obter token: ${response.statusText}`);
        const data = await response.json();
        cachedToken = { token: data.token, expiresAt: Date.now() + TOKEN_EXPIRATION_MINUTES * 60 * 1000 };
        console.log("üîë Token obtido com sucesso.");
      } else {
        console.log("üîë Usando token em cache.");
      }

      const ai = new GoogleGenAI({ apiKey: cachedToken.token });

      const connectionConfig = {
        model: MODEL_NAME,
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
              voiceConfig: {
                prebuiltVoiceConfig: {
                  voiceName: 'Puck',
                }
              }
            },
        },
        callbacks: {
          onopen: () => {
            console.log("‚úÖ Live API conectada!");
            setIsConnected(true);
            setIsLoading(false);
            reconnectAttemptsRef.current = 0;
          },
          onmessage: handleServerMessage,
          onerror: (e: any) => {
            console.error("‚ùå Erro na Live API:", e.message || e);
            setError(e.message || "Ocorreu um erro na conex√£o.");
            setIsLoading(false);
            setIsConnected(false);
          },
          onclose: (e: CloseEvent) => {
            console.log(`üîå Live API desconectada (Code: ${e.code}, Reason: ${e.reason}, Clean: ${e.wasClean})`);
            setIsConnected(false);
            setIsLoading(false);
            sessionRef.current = null;
            if (!e.wasClean && reconnectAttemptsRef.current < MAX_RECONNECT_ATTEMPTS) {
              reconnectAttemptsRef.current++;
              console.log(`üîÑ Tentando reconectar (${reconnectAttemptsRef.current}/${MAX_RECONNECT_ATTEMPTS})...`);
              setTimeout(() => connect(), 2000);
            }
          },
        },
      };

      console.log("üì° Configura√ß√£o da conex√£o:", JSON.stringify(connectionConfig.config, null, 2));
      sessionRef.current = await ai.live.connect(connectionConfig);

    } catch (e) {
      const err = e as Error;
      console.error("‚ùå Falha fatal ao conectar:", err);
      setError(err.message);
      setIsLoading(false);
    }
  }, [handleServerMessage, systemInstruction]);

  // --- 3. Captura e Envio de √Åudio ---
  const startAudioCapture = useCallback(async () => {
    if (isRecording) return;
    
    if (!sessionRef.current || !isConnected) {
      console.warn("Tentativa de gravar sem conex√£o. Conectando primeiro...");
      await connect();
      // Aguarda a conex√£o ser estabelecida antes de continuar
      await new Promise<void>(resolve => {
        const interval = setInterval(() => {
          if (sessionRef.current && isConnected) {
            clearInterval(interval);
            resolve();
          }
        }, 100);
      });
    }

    console.log("üé§ Iniciando captura de √°udio...");
    try {
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: SEND_SAMPLE_RATE });
      console.log(`üéß AudioContext criado com sampleRate: ${audioContextRef.current.sampleRate}Hz`);
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;
      const source = audioContextRef.current.createMediaStreamSource(stream);
      scriptProcessorRef.current = audioContextRef.current.createScriptProcessor(CHUNK_SIZE, 1, 1);

      scriptProcessorRef.current.onaudioprocess = (event) => {
        if (!sessionRef.current || !isConnected) return;
        
        const inputData = event.inputBuffer.getChannelData(0);
        const pcmData = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          pcmData[i] = Math.max(-1, Math.min(1, inputData[i])) * 32767;
        }
        
        let binary = '';
        const bytes = new Uint8Array(pcmData.buffer);
        for (let i = 0; i < bytes.byteLength; i++) {
          binary += String.fromCharCode(bytes[i]);
        }
        const base64Audio = window.btoa(binary);

        try {
          // Estrutura correta alinhada com a API oficial
          sessionRef.current.sendClientContent({
            turns: [
              {
                role: "user",
                parts: [
                  {
                    inlineData: {
                      mimeType: `audio/pcm;rate=${SEND_SAMPLE_RATE}`,
                      data: base64Audio,
                    },
                  },
                ],
              },
            ],
          });
        } catch (e) {
          console.error("‚ùå Erro ao enviar √°udio:", e);
        }
      };

      source.connect(scriptProcessorRef.current);
      scriptProcessorRef.current.connect(audioContextRef.current.destination);
      setIsRecording(true);
    } catch (e) {
      const err = e as Error;
      console.error("‚ùå Falha ao iniciar captura de √°udio:", err);
      setError("Permiss√£o de microfone negada ou dispositivo n√£o encontrado.");
    }
  }, [isConnected, isRecording, connect]);

  // --- 4. Parar Captura de √Åudio ---
  const stopAudioCapture = useCallback(() => {
    if (!isRecording) return;
    console.log("üõë Parando captura de √°udio...");

    mediaStreamRef.current?.getTracks().forEach(track => track.stop());
    scriptProcessorRef.current?.disconnect();
    audioContextRef.current?.close();
    
    mediaStreamRef.current = null;
    scriptProcessorRef.current = null;
    audioContextRef.current = null;

    // A chamada expl√≠cita para finalizar o turno foi removida.
    // A API √© projetada para detetar o fim do stream de √°udio automaticamente
    // quando a captura para, o que resolve os erros de tipo persistentes.
    console.log("üèÅ Captura de √°udio parada. A API ir√° processar o turno.");

    setIsRecording(false);
  }, [isRecording]);

  // --- 5. Fun√ß√µes de Controlo ---
  const toggleRecording = useCallback(() => {
    if (isRecording) {
      stopAudioCapture();
    } else {
      startAudioCapture();
    }
  }, [isRecording, startAudioCapture, stopAudioCapture]);

  const closeSession = useCallback(() => {
    console.log("üö™ Fechando sess√£o...");
    stopAudioCapture();
    sessionRef.current?.close();
    sessionRef.current = null;
  }, [stopAudioCapture]);

  // --- 6. Efeito de Limpeza ---
  useEffect(() => {
    return () => {
      closeSession();
    };
  }, []); // eslint-disable-line react-hooks/exhaustive-deps

  return {
    connect,
    toggleRecording,
    closeSession, // Adicionado
    stopAudioCapture, // Adicionado
    isConnected,
    isRecording,
    isLoading,
    error,
  };
}
